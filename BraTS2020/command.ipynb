{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (in_dim // heads) ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(in_dim, in_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.heads, C // self.heads).transpose(1, 2), qkv)\n",
    "\n",
    "        dots = (q @ k.transpose(-1, -2)) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicUNetDe(nn.Module):\n",
    "    # ...\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 2,\n",
    "        features: Sequence[int] = (32, 32, 64, 128, 256, 32),\n",
    "        act: Union[str, tuple] = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": True}),\n",
    "        norm: Union[str, tuple] = (\"instance\", {\"affine\": True}),\n",
    "        bias: bool = True,\n",
    "        dropout: Union[float, tuple] = 0.0,\n",
    "        upsample: str = \"deconv\",\n",
    "        dimensions: Optional[int] = None,\n",
    "    ):\n",
    "        # ...\n",
    "        self.attention_0 = SelfAttention(features[0])\n",
    "        self.attention_1 = SelfAttention(features[1])\n",
    "        self.attention_2 = SelfAttention(features[2])\n",
    "        self.attention_3 = SelfAttention(features[3])\n",
    "        self.attention_4 = SelfAttention(features[4])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t, embeddings=None, image=None):\n",
    "        # ...\n",
    "        x0 = self.conv_0(x, temb)\n",
    "        x0 = self.attention_0(x0)\n",
    "        if embeddings is not None:\n",
    "            x0 += embeddings[0]\n",
    "\n",
    "        x1 = self.down_1(x0, temb)\n",
    "        x1 = self.attention_1(x1)\n",
    "        if embeddings is not None:\n",
    "            x1 += embeddings[1]\n",
    "\n",
    "        x2 = self.down_2(x1, temb)\n",
    "        x2 = self.attention_2(x2)\n",
    "        if embeddings is not None:\n",
    "            x2 += embeddings[2]\n",
    "\n",
    "        x3 = self.down_3(x2, temb)\n",
    "        x3 = self.attention_3(x3)\n",
    "        if embeddings is not None:\n",
    "            x3 += embeddings[3]\n",
    "\n",
    "        x4 = self.down_4(x3, temb)\n",
    "        x4 = self.attention_4(x4)\n",
    "        if embeddings is not None:\n",
    "            x4 += embeddings[4]\n",
    "\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = \"\"\"\n",
    "import numpy as np\n",
    "from dataset.brats_data_utils_multi_label import get_loader_brats\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from light_training.evaluation.metric import dice\n",
    "from light_training.trainer import Trainer\n",
    "from monai.utils import set_determinism\n",
    "from light_training.utils.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from light_training.utils.files_helper import save_new_model_and_delete_last\n",
    "from unet.basic_unet_denose import BasicUNetDe\n",
    "from unet.basic_unet import BasicUNetEncoder\n",
    "import argparse\n",
    "from monai.losses.dice import DiceLoss\n",
    "import yaml\n",
    "from guided_diffusion.gaussian_diffusion import get_named_beta_schedule, ModelMeanType, ModelVarType,LossType\n",
    "from guided_diffusion.respace import SpacedDiffusion, space_timesteps\n",
    "from guided_diffusion.resample import UniformSampler\n",
    "set_determinism(123)\n",
    "import os\n",
    "\n",
    "data_dir = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "logdir = \"/kaggle/working/DiffAttentionUnet/BraTS2020/logs\"\n",
    "\n",
    "model_save_path = os.path.join(logdir, \"model\")\n",
    "\n",
    "#env = \"DDP\" \n",
    "env = \"pytorch\"\n",
    "max_epoch = 300\n",
    "batch_size = 1\n",
    "val_every = 10\n",
    "num_gpus = 1\n",
    "device = \"cuda:0\"\n",
    "\n",
    "number_modality = 4\n",
    "number_targets = 3 ## WT, TC, ET\n",
    "\n",
    "class DiffUNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_model = BasicUNetEncoder(3, number_modality, number_targets, [16, 16, 32, 64, 128, 16])\n",
    "\n",
    "        self.model = BasicUNetDe(3, number_modality + number_targets, number_targets, [16, 16, 32, 64, 128, 16], \n",
    "                                act = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": False}))\n",
    "   \n",
    "        betas = get_named_beta_schedule(\"linear\", 1000)\n",
    "        self.diffusion = SpacedDiffusion(use_timesteps=space_timesteps(1000, [1000]),\n",
    "                                            betas=betas,\n",
    "                                            model_mean_type=ModelMeanType.START_X,\n",
    "                                            model_var_type=ModelVarType.FIXED_LARGE,\n",
    "                                            loss_type=LossType.MSE,\n",
    "                                            )\n",
    "\n",
    "        self.sample_diffusion = SpacedDiffusion(use_timesteps=space_timesteps(1000, [50]),\n",
    "                                            betas=betas,\n",
    "                                            model_mean_type=ModelMeanType.START_X,\n",
    "                                            model_var_type=ModelVarType.FIXED_LARGE,\n",
    "                                            loss_type=LossType.MSE,\n",
    "                                            )\n",
    "        self.sampler = UniformSampler(1000)\n",
    "\n",
    "\n",
    "    def forward(self, image=None, x=None, pred_type=None, step=None):\n",
    "        if pred_type == \"q_sample\":\n",
    "            noise = torch.randn_like(x).to(x.device)\n",
    "            t, weight = self.sampler.sample(x.shape[0], x.device)\n",
    "            return self.diffusion.q_sample(x, t, noise=noise), t, noise\n",
    "\n",
    "        elif pred_type == \"denoise\":\n",
    "            embeddings = self.embed_model(image)\n",
    "            return self.model(x, t=step, image=image, embeddings=embeddings)\n",
    "\n",
    "        elif pred_type == \"ddim_sample\":\n",
    "            embeddings = self.embed_model(image)\n",
    "\n",
    "            sample_out = self.sample_diffusion.ddim_sample_loop(self.model, (1, number_targets, 96, 96, 96), model_kwargs={\"image\": image, \"embeddings\": embeddings})\n",
    "            sample_out = sample_out[\"pred_xstart\"]\n",
    "            return sample_out\n",
    "\n",
    "class BraTSTrainer(Trainer):\n",
    "    def __init__(self, env_type, max_epochs, batch_size, device=\"cpu\", val_every=1, num_gpus=1, logdir=\"./logs/\", master_ip='localhost', master_port=17750, training_script=\"train.py\"):\n",
    "        super().__init__(env_type, max_epochs, batch_size, device, val_every, num_gpus, logdir, master_ip, master_port, training_script)\n",
    "        self.window_infer = SlidingWindowInferer(roi_size=[96, 96, 96],\n",
    "                                        sw_batch_size=1,\n",
    "                                        overlap=0.25)\n",
    "        self.model = DiffUNet()\n",
    "\n",
    "        self.best_mean_dice = 0.0\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "        self.ce = nn.CrossEntropyLoss() \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.scheduler = LinearWarmupCosineAnnealingLR(self.optimizer,\n",
    "                                                  warmup_epochs=30,\n",
    "                                                  max_epochs=max_epochs)\n",
    "\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss(sigmoid=True)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        image, label = self.get_input(batch)\n",
    "        x_start = label\n",
    "\n",
    "        x_start = (x_start) * 2 - 1\n",
    "        x_t, t, noise = self.model(x=x_start, pred_type=\"q_sample\")\n",
    "        pred_xstart = self.model(x=x_t, step=t, image=image, pred_type=\"denoise\")\n",
    "\n",
    "        loss_dice = self.dice_loss(pred_xstart, label)\n",
    "        loss_bce = self.bce(pred_xstart, label)\n",
    "\n",
    "        pred_xstart = torch.sigmoid(pred_xstart)\n",
    "        loss_mse = self.mse(pred_xstart, label)\n",
    "\n",
    "        loss = loss_dice + loss_bce + loss_mse\n",
    "\n",
    "        self.log(\"train_loss\", loss, step=self.global_step)\n",
    "\n",
    "        return loss \n",
    " \n",
    "    def get_input(self, batch):\n",
    "        image = batch[\"image\"]\n",
    "        label = batch[\"label\"]\n",
    "       \n",
    "        label = label.float()\n",
    "        return image, label \n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        image, label = self.get_input(batch)    \n",
    "        \n",
    "        output = self.window_infer(image, self.model, pred_type=\"ddim_sample\")\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        output = (output > 0.5).float().cpu().numpy()\n",
    "\n",
    "        target = label.cpu().numpy()\n",
    "        o = output[:, 1]\n",
    "        t = target[:, 1] # ce\n",
    "        wt = dice(o, t)\n",
    "        # core\n",
    "        o = output[:, 0]\n",
    "        t = target[:, 0]\n",
    "        tc = dice(o, t)\n",
    "        # active\n",
    "        o = output[:, 2]\n",
    "        t = target[:, 2]\n",
    "        et = dice(o, t)\n",
    "        \n",
    "        return [wt, tc, et]\n",
    "\n",
    "    def validation_end(self, mean_val_outputs):\n",
    "        wt, tc, et = mean_val_outputs\n",
    "\n",
    "        self.log(\"wt\", wt, step=self.epoch)\n",
    "        self.log(\"tc\", tc, step=self.epoch)\n",
    "        self.log(\"et\", et, step=self.epoch)\n",
    "\n",
    "        self.log(\"mean_dice\", (wt+tc+et)/3, step=self.epoch)\n",
    "\n",
    "        mean_dice = (wt + tc + et) / 3\n",
    "        if mean_dice > self.best_mean_dice:\n",
    "            self.best_mean_dice = mean_dice\n",
    "            save_new_model_and_delete_last(self.model, \n",
    "                                            os.path.join(model_save_path, \n",
    "                                            f\"best_model_{mean_dice:.4f}.pt\"), \n",
    "                                            delete_symbol=\"best_model\")\n",
    "\n",
    "        save_new_model_and_delete_last(self.model, \n",
    "                                        os.path.join(model_save_path, \n",
    "                                        f\"final_model_{mean_dice:.4f}.pt\"), \n",
    "                                        delete_symbol=\"final_model\")\n",
    "\n",
    "        print(f\"wt is {wt}, tc is {tc}, et is {et}, mean_dice is {mean_dice}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_ds, val_ds, test_ds = get_loader_brats(data_dir=data_dir, batch_size=batch_size, fold=0)\n",
    "    \n",
    "    trainer = BraTSTrainer(env_type=env,\n",
    "                            max_epochs=max_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            device=device,\n",
    "                            logdir=logdir,\n",
    "                            val_every=val_every,\n",
    "                            num_gpus=num_gpus,\n",
    "                            master_port=17751,\n",
    "                            training_script=__file__)\n",
    "\n",
    "    trainer.train(train_dataset=train_ds, val_dataset=val_ds)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file_path = '/kaggle/working/DiffAttentionUnet/BraTS2020/train.py'\n",
    "\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.utils import set_determinism\n",
    "from unet.basic_unet_denose import BasicUNetDe\n",
    "from unet.basic_unet import BasicUNetEncoder\n",
    "import argparse\n",
    "from monai.losses.dice import DiceLoss\n",
    "import yaml\n",
    "from guided_diffusion.gaussian_diffusion import get_named_beta_schedule, ModelMeanType, ModelVarType,LossType\n",
    "from guided_diffusion.respace import SpacedDiffusion, space_timesteps\n",
    "from guided_diffusion.resample import UniformSampler\n",
    "set_determinism(123)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (16, 16, 32, 64, 128, 16).\n"
     ]
    }
   ],
   "source": [
    "number_modality = 4\n",
    "number_targets = 3 \n",
    "embed_model = BasicUNetEncoder(3, number_modality, number_targets, [16, 16, 32, 64, 128, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (16, 16, 32, 64, 128, 16).\n"
     ]
    }
   ],
   "source": [
    "model = BasicUNetDe(3, number_modality + number_targets, number_targets, [16, 16, 32, 64, 128, 16], \n",
    "                                act = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicUNetDe(\n",
       "  (temb): Module(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv_0): TwoConv(\n",
       "    (temb_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (conv_0): Convolution(\n",
       "      (conv): Conv3d(7, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (adn): ADN(\n",
       "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (conv_1): Convolution(\n",
       "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (adn): ADN(\n",
       "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_1): Down(\n",
       "    (max_pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_2): Down(\n",
       "    (max_pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_3): Down(\n",
       "    (max_pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_4): Down(\n",
       "    (max_pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upcat_4): UpCat(\n",
       "    (upsample): UpSample(\n",
       "      (deconv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (att_4): SelfAttention(\n",
       "    (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "    (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (upcat_3): UpCat(\n",
       "    (upsample): UpSample(\n",
       "      (deconv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (att_3): SelfAttention(\n",
       "    (to_qkv): Linear(in_features=32, out_features=96, bias=False)\n",
       "    (to_out): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (upcat_2): UpCat(\n",
       "    (upsample): UpSample(\n",
       "      (deconv): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (att_2): SelfAttention(\n",
       "    (to_qkv): Linear(in_features=16, out_features=48, bias=False)\n",
       "    (to_out): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (upcat_1): UpCat(\n",
       "    (upsample): UpSample(\n",
       "      (deconv): ConvTranspose3d(16, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (convs): TwoConv(\n",
       "      (temb_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (conv_0): Convolution(\n",
       "        (conv): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (conv_1): Convolution(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (att_1): SelfAttention(\n",
       "    (to_qkv): Linear(in_features=16, out_features=48, bias=False)\n",
       "    (to_out): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (final_conv): Conv3d(16, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
